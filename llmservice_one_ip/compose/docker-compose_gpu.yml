version: "3.9"

name: nchcsystem

networks:
  nchc:
    external: false
    
services:

  nchc_qdrant:
    container_name: nchc_qdrant
    image: qdrant/qdrant:v1.8.4
    pull_policy: always    
    ports:
      - ${nchc_qdrant}:6333
    volumes:
      - ./storage/qdrant_data:/qdrant_data
    environment:
      - QDRANT__SERVICE__API_KEY=${SYSTEM_KEY}
    restart: always
    networks:
      - nchc 
      
  nchc_ollama:
    container_name: nchc_ollama
    image: ollama/ollama:latest
    pull_policy: always
    ports:
      - ${nchc_ollama}:11434    
    volumes:
      - ./storage/ollama_data:/root/.ollama
      - ./storage/output:/output          
    tty: true
    restart: unless-stopped
    networks:
      - nchc
    deploy:
      resources:
        #limits:
        #  cpus: '4.50'
        #  memory: 5000M       
        reservations:
          devices:
            - driver: nvidia
              count: "all"
              capabilities: [gpu]
              
  nchc_openwebui:
    container_name: nchc_openwebui
    image: ghcr.io/open-webui/open-webui:main
    pull_policy: always
    ports:
      - ${nchc_openwebui}:8080
    volumes:
      - ./storage/openwebui_data:/app/backend/data
    restart: unless-stopped
    networks:
      - nchc
    depends_on:
      - nchc_ollama      
    environment:
      - '/ollama/api=http://nchc_ollama:11434/api'
      - 'OLLAMA_BASE_URL=${OLLAMA_BASE_URL}'
      - 'OPENAI_API_BASE_URLS=${OPENAI_API_BASE_URLS}'
      - 'OPENAI_API_KEYS=${OPENAI_API_KEYS}'      
    extra_hosts:
      - host.docker.internal:host-gateway
              
  nchc_anythingllm:
    container_name: nchc_anythingllm
    image: mintplexlabs/anythingllm
    pull_policy: always
    ports:
      - ${nchc_anythingllm}:3001    
    volumes:
      - ./storage/anythingllm_data:/app/server/storage
      - ./storage/anythingllm_hotdir:/app/collector/hotdir
      - ./storage/anythingllm_outputs:/app/collector/outputs
      - ./storage/anythingllm_env.txt:/app/server/.env
    cap_add:
      - SYS_ADMIN
    user: "${UID:-1000}:${GID:-1000}"
    env_file:
      - .env	  
    networks:
      - nchc    
    extra_hosts:
      - host.docker.internal:host-gateway   
    environment:
      - STORAGE_DIR=/app/server/storage
      - SERVER_PORT=3001
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: "all"
              capabilities: [gpu]

  nchc_php:
    container_name: nchc_php
    image: php:8.3-fpm
    pull_policy: always    
    ports:
      - ${nchc_php}:9000
    volumes:
      - ./website:/var/www/html      
    restart: always
    networks:
      - nchc     
    extra_hosts:
      - host.docker.internal:host-gateway
      
  nchc_nginx:
    container_name: nchc_nginx
    image: nginx
    ports:
      -  ${nchc_nginx_80}:80
    volumes:
      - ./nginx/default.conf:/etc/nginx/conf.d/default.conf 
      - ./website:/var/www/html      
    restart: always
    networks:
        - nchc
    depends_on:
      - nchc_openwebui         
    extra_hosts:
      - host.docker.internal:host-gateway
